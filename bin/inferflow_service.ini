[main]
http_port = 8080
worker_count = 32
is_study_mode = false

default_prompt_template = {query}{\n}{res_prefix}

[transformer_engine]
;models = llama2_7b_hf
;models = llama2_7b_chat_hf
;models = llama2_7b_chat.gguf
;models = llama2_13b_chat_hf
;models = open_llama_3b
;models = bloomz_3b
;models = baichuan2_7b_chat
;models = baichuan2_13b_chat
;models = mistral_7b_instruct
;models = falcon_7b_instruct
;models = falcon_40b_instruct
models = phi_2
;models = aquila_chat2_34b
;models = chatglm2_6b
;models = xverse_13b_chat
;models = yi_6b_200k
;models = yi_34b_chat
;models = internlm_chat_20b
;models = stories_15m.llama2_c
;models = stories_110m.llama2_c
;models = facebook_m2m100_418m
;models = bert_base_multilingual_cased

;devices = 0;1
devices = 0
decoder_cpu_layer_count = 0
cpu_threads = 8

max_concurrent_queries = 6

return_output_tensors = true

;debug options
is_study_mode = false
show_tensors = false

[model.llama2_7b_hf]
model_dir = ${data_root_dir}../models/llama2_7b_hf/
model_specification_file = model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

;max_context_len = 512

;prompt_template = {_query} {res_prefix}
prompt_template = {bos}{query}
;prompt_template = llama2_chat.std
;prompt_template = llama_cpp_t0

[model.llama2_7b_chat_hf]
model_dir = ${data_root_dir}../models/llama2_7b_chat_hf/
model_specification_file = model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

;prompt_template = {_query} {res_prefix}
prompt_template = <s>[INST]{query}[/INST]
;prompt_template = llama2_chat.std
;prompt_template = llama_cpp_t0

[model.llama2_7b_chat.gguf]
model_dir = ${data_root_dir}../models/llama2_7b_chat.gguf/
model_specification_file = model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = <s>[INST]{query}[/INST]

[model.llama2_13b_chat_hf]
model_dir = ${data_root_dir}../models/llama2_13b_chat_hf/
model_specification_file = model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = <s>[INST]{\n}{query}[/INST]{\n}
;prompt_template = llama2_chat.std

[model.open_llama_3b]
model_dir = ${data_root_dir}../models/open_llama_3b/
model_specification_file = model_spec.json

device_weight_data_type = F16
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = {query} {res_prefix}

[model.stories_15m.llama2_c]
model_dir = ${data_root_dir}../models/llama2.c/
model_specification_file = model_spec_15m.json

device_weight_data_type =
host_kv_cache_percent = 0

prompt_template = {bos}{query}

[model.stories_110m.llama2_c]
model_dir = ${data_root_dir}../models/llama2.c/
model_specification_file = model_spec_110m.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0

decoding_strategy = sample.std
prompt_template = {bos}{query}

[model.bloomz_3b]
model_dir = ${data_root_dir}../models/bloomz_3b/
;model_specification_file = model_spec.pickle.json
model_specification_file = model_spec.safetensors.json

device_weight_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

;devices = 0

decoding_strategy = sample.fsd
decoder_input_template = {query}{res_prefix}
;prompt_template = {query} {res_prefix}

[model.baichuan2_7b_chat]
model_dir = ${data_root_dir}../models/baichuan2_7b_chat/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
;device_weight_data_type = F16
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

;max_context_len = 1024

decoding_strategy = {"name":"sample.top_p", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = {user_token}{query}{assistant_token}{res_prefix}
;prompt_template = llama2_chat.std

[model.baichuan2_13b_chat]
model_dir = ${data_root_dir}../models/baichuan2_13b_chat/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q5
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

;{BY_LAYER, BY_TENSOR}
;multi_gpu_strategy = BY_TENSOR

prompt_template = {user_token}{query}{assistant_token}{res_prefix}
;prompt_template = llama2_chat.std

[model.mistral_7b_instruct]
model_dir = ${data_root_dir}../models/mistral_7b_instruct_v0.2/
model_specification_file = model_spec.safetensors.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8
;device_weight_data_type = Q3H
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

devices = 0

decoding_strategy = sample.top_p
prompt_template = <s> [INST] {query} [/INST]

[model.falcon_7b_instruct]
model_dir = ${data_root_dir}../models/falcon_7b_instruct/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = F16
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = sample.fsd
prompt_template = {query}

[model.falcon_40b_instruct]
model_dir = ${data_root_dir}../models/falcon_40b_instruct/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q3H
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 100
be_host_embeddings = true

;devices = 0&1;2&3

decoding_strategy = sample.top_p
prompt_template = {query}

[model.phi_2]
model_dir = ${data_root_dir}../models/phi_2/
model_specification_file = model_spec.safetensors.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

;devices = 0&1;2&3

decoding_strategy = sample.top_p
prompt_template = Instruct: {query}{\n}Output:

[model.aquila_chat2_34b]
model_dir = ${data_root_dir}../models/aquila_chat2_34b/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q3H
;device_weight_data_type = Q8_B32T2
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 100
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = {query}

[model.chatglm2_6b]
model_dir = ${data_root_dir}../models/chatglm2_6b/
model_specification_file = model_spec.json

;weight data types: {F32, F16, Q8, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F32, F16, Q8}
device_weight_data_type = F16
device_kv_cache_data_type = F16
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.fsd", "top_p":0.9}
prompt_template = {#64790}{#64792} [Round 1]{\n}{\n}问：{query}{\n}{\n}答：

[model.xverse_13b_chat]
model_dir = ${data_root_dir}../models/xverse_13b_chat/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = {query}

[model.yi_6b_200k]
model_dir = ${data_root_dir}../models/yi_6b_200k/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = F16
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.fsd", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = {bos}{query}

[model.yi_34b_chat]
model_dir = ${data_root_dir}../models/yi_34b_chat/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q3H
;device_weight_data_type = Q8_B32T2
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 50
be_host_embeddings = true

max_context_len = 4096

decoding_strategy = {"name":"sample.top_p", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = <|im_start|>system{\n}{system_prompt}<|im_end|>{\n}<|im_start|>user{\n}{query}<|im_end|>{\n}<|im_start|>assistant{\n}

[model.internlm_chat_20b]
model_dir = ${data_root_dir}../models/internlm_chat_20b/
model_specification_file = model_spec.json

;{F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q4
;{F16, Q8}
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 100
be_host_embeddings = true

decoding_strategy = sample.top_p
prompt_template = <s><|User|>:{query}\n<|Bot|>:

[model.facebook_m2m100_418m]
model_dir = ${data_root_dir}../models/facebook_m2m100_418m/
model_specification_file = model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8_B32T2
host_kv_cache_percent = 0
be_host_embeddings = true

;{BY_LAYER, BY_TENSOR}
;multi_gpu_strategy = BY_TENSOR

decoding_strategy = {"name":"sample.std", "top_p":0.9, "eos_bypassing_max":0}
encoder_input_template = {__zh__}{ }{query}{</s>}
decoder_input_template = {#2}{__en__}{res_prefix}
;encoder_input_template = {__en__}{ }{query}{</s>}
;decoder_input_template = {#2}{__fr__}{res_prefix}
;prompt_template = {query}{res_prefix}

[model.bert_base_multilingual_cased]
model_dir = ${data_root_dir}../models/bert_base_multilingual_cased/
model_specification_file = model_spec.json

encoder_input_template = {[CLS]}{query}{[SEP]}

[prompt_templates]
prompt_template_count = 10
prompt_template1 = simple
prompt_template2 = llama2_chat.std
prompt_template3 = llama2_chat.std.1
prompt_template4 = llama2_chat.std.2
prompt_template5 = llama2_chat.alt
prompt_template6 = llama2_chat.alt.2
prompt_template7 = vicuna-1.0
prompt_template8 = vicuna-1.1
prompt_template9 = llama_cpp_t0
prompt_template10 = llama_cpp_t1

[prompt_template.simple]
line_count = 2
line1 = {query}
line2 = {res_prefix}

[prompt_template.llama2_chat.std]
line_count = 2
line1 = <s>[INST]
line2 = {query}[/INST]

[prompt_template.llama2_chat.std.1]
line_count = 4
line1 = <s>[INST]<<SYS>>
line2 = {system_prompt}
line3 = <</SYS>>
line4 = {query}[/INST]

[prompt_template.llama2_chat.std.2]
line_count = 5
line1 = <s>[INST]<<SYS>>
line2 = You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.
line3 = If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
line4 = <</SYS>>
line5 = {query}[/INST]

[prompt_template.llama2_chat.alt]
line_count = 2
line1 = USER: {query}
line2 = ASSISTANT: {res_prefix}

[prompt_template.llama2_chat.alt.2]
line_count = 3
line1 = SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
line2 = USER: {query}
line3 = ASSISTANT: {res_prefix}

[prompt_template.vicuna-1.0]
line_count = 4
line1 = ### Human:
line2 = {query}
line3 = ### Assistant:
line4 = {res_prefix}

[prompt_template.vicuna-1.1]
line_count = 2
line1 = USER: {query}
line2 = ASSISTANT: {res_prefix}

[prompt_template.llama_cpp_t0]
line_count = 1
line1 = \n\n### Instruction:\n\n{query}\n\n### Response:\n\n{res_prefix}

[prompt_template.llama_cpp_t1]
line_count = 1 
line1 = \n\n### Instruction:\n\n{_query}\n\n### Response:\n\n{res_prefix}

;//////////////////////////////////////////////////////////////////////////////
;// Application Environment
;//////////////////////////////////////////////////////////////////////////////

[app_env.base]
data_root_dir = ${config_dir}../data/inferflow/
require_enter_key_to_exit = true

[app_env.logging]
enable_logging = 1
log_dir = ${data_root_dir}logs
log_name = ${app_name}

[app_env.status_manager]
enable_monitoring = 0
status_file = ${data_root_dir}/${app_name}.status
listening_port = 9877
